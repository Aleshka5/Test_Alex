<h1>Test_task1:scrapping images from the site</h1>
<h2>Первые шаги</h2>
Сначала чтобы получить данные с этого сайта я решил попробовать requests. Это было не актуально по двум причинам:<br>
1) Сайт имеет динамическую подгрузку элементов разметки для своих товаров.<br>
2) Для доступа на сайт нужно воспользоваться proxy (У меня это не вышло для данного сайта).<br>
Исходя из этого появилась необходимость использовать Selenium как основной инструмент парсинга сайта.
<h2>Про Selenium</h2>
Я имел опыт работы с Selenium, но тут пришлось прикрутить к нему proxy сервер, так как <a href="https://www.ralphlauren.nl/en/men/clothing/hoodies-sweatshirts/10204?webcat=men%7Cclothing%7Cmen-clothing-hoodies-sweatshirts">сайт</a> заблокирован для нашего региона. К сожалению, из-за медленной скорости работы бесплатные варианты не подошли и мне пришлось арендовать IPv4 proxy на одном из сайтов.
Осказалось, что у такого варианта есть свои сложности. Мой новый proxy имел необходимость в аунтификации. Достаточно тяжело самостоятельно написать решение этой проблемы, поэтому я обратился к StackOverfolw. Там я нашёл нужное мне решение, у меня оно находится в модуле solve_auntithication. Ссылка на это решение находится <a href="https://stackoverflow.com/questions/55582136/how-to-set-proxy-with-authentication-in-selenium-chromedriver-python">здесь</a>. Оно помогло мне достоточно быстро подключаться к нужному сайту.<br>
Сам же сайт я понял как парсить, когда нажал на кнопку "Посмотреть больше" и заглянул в адресную строку. Там я увидел в конце строки следующее:" ...&start=128&sz=6 ". Оказалось Start - индекс элемента с которого выводятся товары, а sz (size) - колличество выводимых товаров начиная с start.<br>
Однако сайт имеет не плохую систему защиты от ботов, по моему мнению. Из-за чего не каждый запрос на сайт проходил успешно. Однако мне удалось сильно увеличить количество удачных подключений, производя их через рандомизированные промежутки времени и генерируя различные headers.
<h2>Парсинг HTML</h2>
Изучая HTML разметку полученную с сайта я заметил, что все интересующие меня элементы хранятся в тегах "picture" с атрибутом tabindex="0". Это здорово упростило работу с BeautifulSoup. Однако в HTML хранятся не сами изображения, а ссылки на них в интернете, об этом в следующем разделе...
<h2>Парсинг изображений с помощь requests</h2>
Имея на руках ссылки на нужные изображения осалось обратиться по ним с помощью requests и получить из них content. Когда я стал обращаться по ссылкам я нашёл несколько фото, у которых фото в паре дублировались. Они были помечены специальным образом. Итого у меня вышло собрать датасет из 110 пар изображений.<br>
Так, я справился с первой частью первого задания.
